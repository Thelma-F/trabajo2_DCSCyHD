---
title: "Trabajo 2"
author: "Thelma"
date: "2023-12-27"
output: word_document
---

# Trabajo Final

*Introducción*

El trabajo final del módulo 5 se basará en utilizar el corpus provisto (cuyas
características se describen a continuación) y recorrer todo el flujo de trabajo visto en
clase: preprocesamiento, generación de distribuciones de frecuencias de palabras y
modelado de tópicos. Prestaremos especial atención en la corrección a la explicación y
justificación de las decisiones tomadas, en tanto muestren manejo de los tópicos vistos
en clase.
Dataset

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias scrapeadas
entre julio y septiembre de 2019 de los siguientes medios de circulación nacional:
- Télam
- La Nación
- Clarín
- Perfil
- Infobae
- MinutoUno
- Página 12

Constituye una muestra aleatoria del corpus construido por Florencia Piñeyrúa para su
tesina de grado “Procesamiento del lenguaje natural aplicado al estudio de tópicos de
noticias de seguridad en Argentina: julio a septiembre 2019”. 

El corpus contiene, las siguientes variables:
- id : identificador de cada documento
- url : link a la noticia original
- fecha : fecha de publicación
- anio : año de publicación
- mes : mes de publicación
- dia : dia de publicación
- medio : medio en el que fue publicado
- orientacion: clasificación -provisoria- de los medios según su línea editorial
predominante (más conservador, más progresista, neutral)
- titulo
- texto

*A partir del corpus deberán:*
- Cargar los datos
- Preprocesarlos (normalizar texto, eliminar stopwords)
- Generar una matriz token-por-fila para cada documento

Abrir librerías

<<<<<<< HEAD

```{r warning=FALSE}

library(tidyverse)
library(tidytext)

```

### 1. Cargamos los datos

```{r warning=FALSE}

noticias <- read_csv('M5_corpus_medios.csv')

print(noticias)

```
### 2. Preprocesamos los datos

#### 2.a. Construimos la tabla de tokens del texto
```{r warning=FALSE}

noticias_tidy <- noticias %>%
  unnest_tokens(output = word, 
                input = texto, 
                token = "words") %>% 
  print()

```

#### 2.b Eliminamos stopwords

Cargamos el diccionario de stopwords

```{r warning=FALSE}

stop_words <- read_csv("https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt", 
                       col_names=FALSE) %>%
  rename(word = X1) %>%
  mutate(word = stringi::stri_trans_general(word, "Latin-ASCII")) 

stop_words <- stop_words %>%
  bind_rows(tibble(word = c('embed', 'leé', 'cronica.com.ar', 'minutouno.com', 'comentar', 'jpg', 
                            'loading', 'pristupluk', '	minutouno.com', 'guardar', '	páginai12', '01',	
                            'l.l', 'loading', 'jpe', 'más', 'también', 'está', 'había', 'qué', 'así', 
                            'están', 'además', 'según', 'día', '1', 'gusta', 'twitter', 
                            'fuente', 'whatsapp', 'compartir', 'facebook', 'mail', 'él'))) %>% 
  print()

```


#### 2.c. Procedemos a la eliminación:
```{r warning=FALSE}

corpus_noticias <- noticias_tidy %>%
  anti_join (stop_words, 
             by = c("word" = "word")) %>% 
  print()

```

### 3. Generar una matriz token-por-fila para cada documento


# Consigna 1) 

a) ¿Cuáles son las palabras más utilizadas en cada uno de los medios? 

b) ¿Pueden verse diferencias? (Tener en cuenta las diferentes métricas trabajadas en el
curso: tf, tf-idf, etc.) 

c) Generar las visualizaciones que considere más pertinentes para responder la pregunta

<<<<<<< HEAD

Armo un corpus de noticias donde se identifiquen las tf, idf y las tf_idf

=======

```{r warning=FALSE}

corpus_tf_idf_medio <- corpus_noticias %>%
  count(medio, word, sort = TRUE) %>%
  bind_tf_idf(word, medio, n) %>% 
  arrange(desc(tf_idf)) %>% 
  print()

```

Agrupo y ordeno por tf_idf


```{r}

top_words_by_medio <- corpus_noticias %>%
  group_by(medio) %>%
  count(medio, word) |> 
  slice_max(n = 40, 
            order_by = n,
            with_ties = FALSE) %>%
  ungroup() |> 
  print()

```

Una opción para ver las palabras más utilizadas en cada medio es: 

```{r}

top_words_by_medio <- corpus_noticias %>%
  group_by(medio, word) %>%
  summarise(tf = n()) %>%
  arrange(medio, desc(tf)) %>%
  group_by(medio) %>%
  top_n(10, wt = tf) %>% 
  print()

```


Otra opción es filtrar por cada medio en particular 

=======

```{r warning=FALSE}
unique(corpus_noticias$medio)
```
#### Clarín

Las palabras más utilizadas de Clarín

```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "clarin") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()

```

#### Perfil

Las palabras más utilizadas de Perfil

```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "perfil") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()

```

#### Minuto Uno

Las palabras más utilizadas de Minutouno

```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "minutouno") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```


#### Infobae

Las palabras más utilizadas de Infobae


```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "infobae") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```


#### La Nación

Las palabras más utilizadas de La Nación


```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "lanacion") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```


#### Página 12

Las palabras más utilizadas de Página 12


```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "pagina12") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```


#### Crónica

Las palabras más utilizadas de Crónica


```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "cronica") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```

#### Telam

Las palabras más utilizadas de Telam


```{r warning=FALSE}
top_words_by_medio <- corpus_tf_idf %>%
  filter(medio == "telam") %>% 
  group_by(medio) %>%
  top_n(20, wt = tf) %>%
  arrange(desc(n)) %>% 
  print()
```

### Gráfico 


Un gráfico que visualice las palabras de mayor importancia

```{r}
# Filtrar solo las 10 palabras principales por medio
top_words_by_medio <- corpus_tf_idf %>%
  group_by(medio) %>%
  top_n(10, wt = tf)

#Paso la primera letra de los medios en mayúscula

top_words_by_medio$medio <- str_to_title(top_words_by_medio$medio)


# Crear el gráfico
ggplot(top_words_by_medio, aes(x = tf, y = reorder(word, tf), fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF-IDF", y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()
```


Un gráfico que visualice las palabras que tengan importancia y sean informativas (tf_idf)

```{r}
# Filtrar solo las 10 palabras principales por medio
top_words_by_medio <- corpus_tf_idf %>%
  group_by(medio) %>%
  top_n(10, wt = tf_idf)


#Paso la primera letra de los medios en mayúscula

top_words_by_medio$medio <- str_to_title(top_words_by_medio$medio)

# Crear el gráfico
ggplot(top_words_by_medio, aes(x = tf_idf, y = reorder(word, tf_idf), fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF-IDF", y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()
```



# Consigna 2) 

¿Cuáles son los tópicos principales en el corpus? 
¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. 
Generar las visualizaciones más adecuadas para responder a las preguntas

Abrimos nuevas librerías

```{r warning=FALSE}
library(topicmodels)
library(tictoc)
```


Topic modeling
```{r}
topicos_4 <- LDA(corpus_noticias, k=4, control = list(seed = 1234))
```


```{r}
noticias_topics <- tidy(topicos_4, matrix = "beta") # Si esta línea les tira algún error, hagan install.packages("reshape2")

#Lo pasamos a porcentajes
noticias_topics %>%
  mutate(beta = round(100*beta,6))
```

```{r}
top_noticias_topics <- noticias_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_noticias_topics %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```


=======


# Consigna 2) 

¿Cuáles son los tópicos principales en el corpus? 
¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. 
Generar las
visualizaciones más adecuadas para responder a las preguntas


>>>>>>> 8f00baa9fa528fb16ff97b78564bf01e526a9a7d
# Consigna 3) 

A continuación, seleccionar las noticias vinculadas a algún tópico relevante (por
ejemplo, “Elecciones”) y construir un clasificador para predecir la orientación del
diario1. 
Utilizar alguno de los modelos de clasificación vistos a lo largo de al
Diplomatura (regresión logística, random forest, etc.). 
Utilizar como features el
“Spanish Billion Word Corpus and Embeddings”, analizado en clase (pueden
descargar el embedding en formato .bin del link). 

¿Qué resultados arroja el modelo? ¿Es posible mediante el texto de las noticias conocer la línea editorial del diario? 
Generar las visualizaciones y tablas correspondientes para una correcta evaluación del modelo.


