---
title: "Trabajo 2"
author: "Thelma, Beatriz e Ignacio"
date: "2023-12-27"
output: word_document
---

Trabajo Final

*Introducción*

El trabajo final del módulo 5 se basará en utilizar el corpus provisto (cuyas
características se describen a continuación) y recorrer todo el flujo de trabajo visto en
clase: preprocesamiento, generación de distribuciones de frecuencias de palabras y
modelado de tópicos. Prestaremos especial atención en la corrección a la explicación y
justificación de las decisiones tomadas, en tanto muestren manejo de los tópicos vistos
en clase.
Dataset

El archivo que se adjunta consiste en un corpus de unas 7.000 noticias scrapeadas
entre julio y septiembre de 2019 de los siguientes medios de circulación nacional:
    - Télam
    - La Nación
    - Clarín
    - Perfil
    - Infobae
    - MinutoUno
    - Página 12

Constituye una muestra aleatoria del corpus construido por Florencia Piñeyrúa para su
tesina de grado “Procesamiento del lenguaje natural aplicado al estudio de tópicos de
noticias de seguridad en Argentina: julio a septiembre 2019”. 

El corpus contiene, las siguientes variables:
    - id : identificador de cada documento
    - url : link a la noticia original
    - fecha : fecha de publicación
    - anio : año de publicación
    - mes : mes de publicación
    - dia : dia de publicación
    - medio : medio en el que fue publicado
    - orientacion: clasificación -provisoria- de los medios según su línea editorial
    predominante (más conservador, más progresista, neutral)
    - titulo
    - texto

*A partir del corpus deberán:*
    - Cargar los datos
    - Preprocesarlos (normalizar texto, eliminar stopwords)
    - Generar una matriz token-por-fila para cada documento

# Se cargan las librerías a emplear

```{r librerias}

library(tidyverse)
library(tidytext)
# library(topicmodels)
# library(tictoc)
library(stm)
library (tidymodels)
# library(reshape2)
#library(textrecipes)
#library(textclean)
#library(rsample)
#library(themis)
#library(parsnip)


```

# Se carga el dataset

```{r base_noticias}

noticias <- read_csv('M5_corpus_medios.csv')

noticias |> 
  slice(1:50) |> 
  print(noticias) |> 
  View()
```

# Preprocesamiento de los datos

## Construimos la tabla de tokens del texto

```{r limpieza_noticias, warning=FALSE}

# Armado del listado de noticias que no contienen texto
noticias_vacias <- noticias |> 
  filter(is.na(texto))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_vacias, by = c("titulo" = "titulo"))

# Armado del listado de noticias con errores en el webscrapeo, ya que inician con "Article 
# download()' failed with" o con 'NoneType' object has no attribute 'find_all'
noticias_mal_descargadas <- noticias |> 
  # El ^ antes de artículo es una expresión regular para marcar que debe buscarse al inicio del título
  filter(str_detect(titulo, "^'NoneType'|^Article"))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_mal_descargadas, by = c("titulo" = "titulo"))

noticias <- noticias |> 
  # Elimina puntuación y valores numéricos y, posteriormente, lleva todo a minúsculas
  mutate(texto = tolower(gsub("[[:punct:]0-9]", "", texto))) %>%
  # Elimina múltiples espaciados erróneamente ingresados
  mutate(texto = str_squish(texto))

# Armado del listado de noticias que, después de limpieza de caracteres, se considera que son  cortas y producen ruido a la hora de evaluar las term frequency (tf)

noticias_cortas <- noticias |> 
  # "\\S+" es la expresión regular para detectar palabras con un espacio
  filter(str_count(texto, "\\S+") < 40)

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_cortas, by = c("titulo" = "titulo"))


# Armado del listado de noticias de La Nación que tuvieron errores en el webscrapeo, ya que inician
# con "envía tu comentario ver legales los comentarios publicados son"

noticias_error_ln <- noticias |> 
  filter(str_detect(texto, "^envía tu comentario ver legales los comentarios publicados son"))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_error_ln, by = c("titulo" = "titulo"))

```

```{r noticias_tidy}

noticias_tidy <- noticias %>%
  unnest_tokens(input = texto, 
                output = word,
                token = "words") 

noticias_tidy |> 
  slice(1:50) |> 
  print()

```

## Cargamos el diccionario de stopwords y agregamos stopwords específicas

```{r diccionario_stopwords}

stop_words <- read_csv("https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt", 
                       col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

stop_words |> 
  slice(1:50) |> 
  print()

stop_words <- stop_words %>%
  bind_rows(tibble(word = c("\tminutouno.com", "\tpáginai12", "a", "además",
                            "además", "ahí", "allí", "ante", "año", "años", "así",
                            "aún", "bajo", "comentar", "comentario", "como", "cómo", "compartir",
                            "compartir", "con", "contra", "cronica.com.ar", "cronicacomar", 
                            "de", "desde", "después", "día", "días", "durante","él", "email", 
                            "embed", "en", "entre", "está", "están", "facebook", "fuente", 
                            "guardar", "gusta", "había", "hacia", "hasta", "jpe", "jpg", "l.l",
                            "leé", "loading", "mail", "más", "mediante", "minutouno.com",
                            "nacion", "páginai", "páginai12", "para", "podría", "por", 
                            "pristupluk", "que", "qué", "según", "será", "si", "sí", "sin", 
                            "sobre", "sólo", "también", "tenía", "través", "twitter", "whatsapp")))

```

## Procedemos a la eliminación de stopwords

```{r anti_join}

corpus_noticias <- noticias_tidy %>%
  anti_join(stop_words, 
            by = c("word" = "word"))

# Elimina ambos archivos porque ya no lo necesitamos
rm(stop_words, noticias_tidy)

corpus_noticias |> 
  slice(1:50) |> 
  print()

```

## Generamos una matriz token-por-fila para cada documento

```{r corpus_tf_idf_medio}

# Creamos un corpus centrado en los medios
corpus_tf_idf_medio <- corpus_noticias %>%
  count(medio, word, 
        sort = TRUE) %>%
  bind_tf_idf(term = word,
              # Al poner como "document" el medio, el idf expresa cuánto usa más un medio que otro 
              # ciertas palabras
              document = medio,
              n = n)

corpus_tf_idf_medio |> 
  slice_max(n = 50,
            order_by = tf,
            # En caso de empates ("tie"), por default la función trae ambos valores; lo seteamos
            # para que sólo traiga uno
            with_ties = FALSE) |> 
  print()

```

# Consigna 1) 

a) ¿Cuáles son las palabras más utilizadas en cada uno de los medios? 

b) ¿Pueden verse diferencias? (tener en cuenta las diferentes métricas trabajadas en el
curso: tf, tf-idf, etc.) 

c) Generar las visualizaciones que considere más pertinentes para responder la pregunta



### Armamos un corpus con las tf, idf y las tf_idf por medio

#### Buscamos las TF (las palabras más importantes/frecuentes)

```{r todos_los_medios}

top_words_tf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = tf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_tf |>   
  print()

```

#### Buscamos las IDF (las palabras más informativas)

```{r}
top_words_idf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = idf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_idf |>   
  print()
```

#### Buscamos las TF_IDF (la frecuencia ponderada o ajustada por la importancia que tiene en el corpus)

```{r}
top_words_tf_idf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = tf_idf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_tf_idf  |>   
  print()
```


## Gráficos

### Gráfico 1: palabras más frecuentes (tf)

```{r grafico_palabras_frecuentes}

# Crear el gráfico
ggplot(top_words_tf, aes(x = tf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "Palabra", 
       y = "TF") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más utilizadas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 1")


```

### Gráfico 2: palabras más informativas (idf)

```{r}
# Crear el gráfico
ggplot(top_words_idf, aes(x = idf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF", 
       y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()+
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más informativas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 2",
       plot.title = element_text(size = 10))


```


### Gráfico 3: palabras palabras de mayor frecuencia ajustadas por la importancia que tiene en el corpus (tf_idf)

```{r grafico_palabras_características}

# Crear el gráfico
ggplot(top_words_tf_idf, aes(x = tf_idf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF", 
       y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()+
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más informativas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 3" )

```

```{r elimina_objetos}

# Elimina objetos que ya no necesitamos
#rm(top_words_by_medio, corpus_tf_idf_medio, noticias_vacias, noticias_mal_descargadas, noticias_cortas, noticias_error_ln)

```



# Consigna 2) 

## a) ¿Cuáles son los tópicos principales en el corpus?

## b) ¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. 

## c) Generar las visualizaciones más adecuadas para responder a las preguntas


## A)


Si vemos los resultados ordenados por tf, se puede observar que existen 6 noticias que arrancan con "Últimas noticias de" y no contienen ninguna palabra (por eso la tf es 1: sólo ahí aparecen espacios vacíos). Deberíamos eliminarlos en el preprocesamiento.

Por otro lado, los resultados ordenados por tf_idf muestra que existen varias "noticias" que son en realidad avisos de descargas fallidas y sus palabras más características son básicamente links sin puntuación. También deben ser eliminados en el preprocesamiento.

```{r corpus_tf_idf_noticia}

# Creamos un corpus, pero esta vez centrados en los títulos
corpus_tf_idf_noticia <- corpus_noticias %>%
  count(titulo, word, 
        sort = TRUE) %>%
  bind_tf_idf(term = word,
              # Al poner como "document" el titulo, el idf expresa cuánto usa más un titulo que otro 
              # ciertas palabras
              document = titulo,
              n = n)

corpus_tf_idf_noticia |> 
  slice_max(n = 500,
            order_by = tf_idf,
            # En caso de empates ("tie"), por default la función trae ambos valores; lo seteamos
            # para que sólo traiga uno
            with_ties = FALSE) |> 
  print()

rm(corpus_noticias)

```


```{r corpus_dtm}

# Creación de la Document Term Matrix (DTM) del corpus de noticias
corpus_dtm <- corpus_tf_idf_noticia %>%
  cast_dtm(term = word,
           document = titulo, 
           value = n)

# La cantidad de documentos y de palabras parece coincidir con la del corpus
corpus_dtm |> 
  print()

```


```{r modelo_topicos}

# corpus_noticias <- terra::na.omit(corpus_dtm)

modelo_LDA7 <- topicmodels::LDA(corpus_dtm, 
                              k = 7, 
                              control = list(seed = 1234567))

```


```{r prob_x_palabra}

                  # Si esta línea les tira algún error, hagan install.packages("reshape2")
palabras_topicos <- tidy(modelo_LDA7, matrix = "beta") 


#Lo pasamos a porcentajes
palabras_topicos %>%
  mutate(beta = round(100*beta, 6))

```


```{r top_palabras_topico}

top_palabras_topicos <- palabras_topicos %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_palabras_topicos %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()

```

```{r principales_noticias_x_topico}

topicos_noticias <- tidy(modelo_LDA7, matrix = "gamma")

# Para observar qué títulos aparecen en las noticias del tópico 2
topicos_noticias |>
  filter(topic == 2) %>%
  filter(gamma > 0.9) |> 
  mutate(gamma = round(gamma, 5)) |> 
  View()

```

```{r noticias_poco_clasificables}

# Para observar cuáles noticias no parecen encajar bien en ninguno de los tópicos posibles
topicos_noticias |> 
  # Ponemos un umbral de bajas chances de encajar en cualquier tópico
  filter(gamma < 0.3) |> 
  # Contamos cuántos son los documentos que cumplen la condición anterior (porque si encajan 
  # perfecto en un tópico, es obvio que en los otros no encajan)
  count(document) |> 
  # Si aparece 5 veces el titulo, es que en los 7 tópicos tiene menos de un 30% de chances
  filter(n == 7) |> 
  View()

```


```{r topicos_x_medio}

topicos_noticias %>%
  rename(titulo = document) %>% 
  # tenemos que renombrar la columna para que pueda hacerse el join
  left_join(noticias %>% select(titulo, medio) %>% unique()) %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)*100) %>%
  ggplot() +
    geom_col(aes(x = topic,
                 y = mean,
                 fill = medio), 
             position='dodge') +
    theme_minimal()

```










STM- pruebas


```{r}
word_counts <- corpus_noticias %>%  
        group_by(id,word) %>%
        summarise(n=n()) %>%
        ungroup()

noticias_dfm <- word_counts %>%
                cast_dfm(id, word, n)

noticias_dfm
```

```{r}
library(readr)
stm_7 <- read_rds('C:/Users/USUARIO/Documents/DCSCyHD/Módulo 5- Minería de texto/clase5/models/stm_15_prev.rds')

metadata <- corpus_noticias %>%  
        group_by(id,medio, titulo ) %>%
        summarise(n=n()) %>%
        ungroup() 


stm_7 <- stm(documents = noticias_dfm,
      K = 7,
      prevalence = ~medio,
      max.em.its = 75, 
      data = metadata,
      init.type = "Spectral")
 write_rds(stm_7, 'C:/Users/USUARIO/Documents/DCSCyHD/Módulo 5- Minería de texto/clase5/models/stm_15_prev.rds')


```



```{r}
betas_stm <- tidy(stm_7, matrix='beta')
doc_2_topics_stm <- tidy(stm_7, matrix='theta')
```


```{r}
betas_stm %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()
```


```{r}
labelTopics(stm_7)
```


```{r}
#Queremos definir cómo está compuesto el tópico 4 porque es el menos claro, para eso le paso la información del modelo y le pido  que me de mayor información de este tópico (los 10 títulos que más alto scorean)

findThoughts(stm_7,
             texts = metadata %>% mutate(titulo = str_sub(titulo, 1, 200)) %>% select(titulo) %>% pull (),
              n=20,
              topics=4)


#Astrología, salud y ciencia (también puede ser misceláneas que se usó en clase)
```

Comparando el modelo LDA contra el modelo STM

```{r}
#LDA

#Beta

palabras_topicos


#Gamma

topicos_noticias

doc_2_topics_lda <- topicos_noticias %>%
  rename(id=document) %>%
  mutate(id=as.integer(id)) %>%
  left_join(noticias_tidy)


doc_2_topics_lda %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)) %>%
  drop_na() %>%
  ggplot(aes(x=medio, y=mean, fill=medio)) + 
    geom_col(position='dodge') +
    facet_wrap(~topic) +
    theme_minimal()

```


```{r}

#STM 


doc_2_topics_stm <- doc_2_topics_stm %>%
  rename(id=document) %>%
  left_join(metadata)

doc_2_topics_stm %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)) %>%
  drop_na() %>%
  ggplot(aes(x=medio, y=mean, fill=medio)) + 
    geom_col(position='dodge') +
    facet_wrap(~topic) +
    theme_minimal()

doc_2_topics_stm %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)) %>%
  drop_na() %>%
  ggplot(aes(x=topic, y=mean, fill=topic)) + 
    geom_col(position='dodge') +
    facet_wrap(~medio) +
    theme_minimal()
```


Justicia y seguridad: LDA (1), STM (5)
Política/elecciones: LDA (5), STM (1)
Economía:  LDA (4), STM (3)
Deportes: LDA (7), STM (7)

El resto de los topicos son más dudosos habria que chequear que comparten. Son LDa 2,3,6


# Consigna 3) 

A continuación, seleccionar las noticias vinculadas a algún tópico relevante (por
ejemplo, “Elecciones”) y construir un clasificador para predecir la orientación del
diario1. 
Utilizar alguno de los modelos de clasificación vistos a lo largo de al
Diplomatura (regresión logística, random forest, etc.). 
Utilizar como features el
“Spanish Billion Word Corpus and Embeddings”, analizado en clase (pueden
descargar el embedding en formato .bin del link). 

¿Qué resultados arroja el modelo? ¿Es posible mediante el texto de las noticias conocer la línea editorial del diario? 
Generar las visualizaciones y tablas correspondientes para una correcta evaluación del modelo.


