---
title: "Trabajo Final- Opción 2"
author: "Ignacio Bastías, Thelma Fratarelli y Beatriz Soria"
date: "2023-12-27"
output: word_document
---

Trabajo Final

*Introducción*

# Se cargan las librerías a emplear

```{r librerias}

library(tidyverse)
library(tidytext)
# library(topicmodels)
# library(tictoc)
library(stm)
library (tidymodels)
# library(reshape2)
#library(textrecipes)
#library(textclean)
#library(rsample)
#library(themis)
#library(parsnip)


```

# Se carga el dataset

```{r base_noticias}

noticias <- read_csv('M5_corpus_medios.csv')

noticias |> 
  slice(1:50) |> 
  print(noticias) |> 
  View()
```

# Preprocesamiento de los datos

## Construimos la tabla de tokens del texto

```{r limpieza_noticias, warning=FALSE}

# Armado del listado de noticias que no contienen texto
noticias_vacias <- noticias |> 
  filter(is.na(texto))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_vacias, by = c("titulo" = "titulo"))

# Armado del listado de noticias con errores en el webscrapeo, ya que inician con "Article 
# download()' failed with" o con 'NoneType' object has no attribute 'find_all'
noticias_mal_descargadas <- noticias |> 
  # El ^ antes de artículo es una expresión regular para marcar que debe buscarse al inicio del título
  filter(str_detect(titulo, "^'NoneType'|^Article"))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_mal_descargadas, by = c("titulo" = "titulo"))

noticias <- noticias |> 
  # Elimina puntuación y valores numéricos y, posteriormente, lleva todo a minúsculas
  mutate(texto = tolower(gsub("[[:punct:]0-9]", "", texto))) %>%
  # Elimina múltiples espaciados erróneamente ingresados
  mutate(texto = str_squish(texto))

# Armado del listado de noticias que, después de limpieza de caracteres, se considera que son  cortas y producen ruido a la hora de evaluar las term frequency (tf)

noticias_cortas <- noticias |> 
  # "\\S+" es la expresión regular para detectar palabras con un espacio
  filter(str_count(texto, "\\S+") < 40)

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_cortas, by = c("titulo" = "titulo"))


# Armado del listado de noticias de La Nación que tuvieron errores en el webscrapeo, ya que inician
# con "envía tu comentario ver legales los comentarios publicados son"

noticias_error_ln <- noticias |> 
  filter(str_detect(texto, "^envía tu comentario ver legales los comentarios publicados son"))

# Usa anti_join para eliminar esas noticias de la base "noticias"
noticias <- noticias |> 
  anti_join(noticias_error_ln, by = c("titulo" = "titulo"))

```

```{r noticias_tidy}

noticias_tidy <- noticias %>%
  unnest_tokens(input = texto, 
                output = word,
                token = "words") 

noticias_tidy |> 
  slice(1:50) |> 
  print()

```

## Cargamos el diccionario de stopwords y agregamos stopwords específicas

```{r diccionario_stopwords}

stop_words <- read_csv("https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt", 
                       col_names=FALSE) %>%
        rename(word = X1) %>%
        mutate(word = stringi::stri_trans_general(word, "Latin-ASCII"))

stop_words |> 
  slice(1:50) |> 
  print()

stop_words <- stop_words %>%
  bind_rows(tibble(word = c("\tminutouno.com", "\tpáginai12", "a", "además",
                            "además", "ahí", "allí", "ante", "año", "años", "así",
                            "aún", "bajo", "comentar", "comentario", "como", "cómo", "compartir",
                            "compartir", "con", "contra", "cronica.com.ar", "cronicacomar", 
                            "de", "desde", "después", "día", "días", "durante","él", "email", 
                            "embed", "en", "entre", "está", "están", "facebook", "fuente", 
                            "guardar", "gusta", "había", "hacia", "hasta", "jpe", "jpg", "l.l",
                            "leé", "loading", "mail", "más", "mediante", "minutouno.com",
                            "nacion", "páginai", "páginai12", "para", "podría", "por", 
                            "pristupluk", "que", "qué", "según", "será", "si", "sí", "sin", 
                            "sobre", "sólo", "también", "tenía", "través", "twitter", "whatsapp")))

```

## Procedemos a la eliminación de stopwords

```{r anti_join}

corpus_noticias <- noticias_tidy %>%
  anti_join(stop_words, 
            by = c("word" = "word"))

# Elimina ambos archivos porque ya no lo necesitamos
rm(stop_words, noticias_tidy)

corpus_noticias |> 
  slice(1:50) |> 
  print()

```

## Generamos una matriz token-por-fila para cada documento

```{r corpus_tf_idf_medio}

# Creamos un corpus centrado en los medios
corpus_tf_idf_medio <- corpus_noticias %>%
  count(medio, word, 
        sort = TRUE) %>%
  bind_tf_idf(term = word,
              # Al poner como "document" el medio, el idf expresa cuánto usa más un medio que otro 
              # ciertas palabras
              document = medio,
              n = n)

corpus_tf_idf_medio |> 
  slice_max(n = 50,
            order_by = tf,
            # En caso de empates ("tie"), por default la función trae ambos valores; lo seteamos
            # para que sólo traiga uno
            with_ties = FALSE) |> 
  print()

```

# Consigna 1) 

a) ¿Cuáles son las palabras más utilizadas en cada uno de los medios? 

b) ¿Pueden verse diferencias? (tener en cuenta las diferentes métricas trabajadas en el
curso: tf, tf-idf, etc.) 

c) Generar las visualizaciones que considere más pertinentes para responder la pregunta



### Armamos un corpus con las tf, idf y las tf_idf por medio

#### Buscamos las TF (las palabras más importantes/frecuentes)

```{r todos_los_medios}

top_words_tf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = tf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_tf |>   
  print()

```

#### Buscamos las IDF (las palabras más informativas)

```{r}
top_words_idf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = idf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_idf |>   
  print()
```

#### Buscamos las TF_IDF (la frecuencia ponderada o ajustada por la importancia que tiene en el corpus)

```{r}
top_words_tf_idf <- corpus_tf_idf_medio %>%
  group_by(medio) %>%
  slice_max(n = 10, 
            # Si seteamos otra variable en "order_by", obtenemos otra respuesta posible
            order_by = tf_idf,
            with_ties = FALSE) %>%
  ungroup()
  
top_words_tf_idf  |>   
  print()
```


## Gráficos


```{r correccion_medios}

#Paso la primera letra de los medios en mayúscula
top_words_by_medio$medio <- str_to_title(top_words_by_medio$medio)

```

### Gráfico 1: palabras más frecuentes (tf)

```{r grafico_palabras_frecuentes}

# Crear el gráfico
ggplot(top_words_by_medio, aes(x = tf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "Palabra", 
       y = "TF") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más utilizadas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 1")


```

### Gráfico 2: palabras más informativas (idf)

```{r}
# Crear el gráfico
ggplot(top_words_idf, aes(x = idf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF", 
       y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()+
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más informativas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 2" )

```


### Gráfico 3: palabras palabras de mayor frecuencia ajustadas por la importancia que tiene en el corpus (tf_idf)

```{r grafico_palabras_características}

# Crear el gráfico
ggplot(top_words_tf_idf, aes(x = tf_idf, 
                               y = fct_reorder(word, desc(tf)), 
                               fill = medio)) +
  geom_col() +
  scale_fill_discrete(name = "Medio") +
  labs(x = "TF", 
       y = "Palabra") +
  facet_wrap(~medio, scales = "free_y") +
  theme_minimal()+
  theme(legend.position = "none") +
  labs(title = "Las 10 palabras más informativas en cada medio",
       subtitle = "julio - septiembre 2019", 
       caption = "Gráfico n° 3" )

```

```{r elimina_objetos}

# Elimina objetos que ya no necesitamos
#rm(top_words_by_medio, corpus_tf_idf_medio, noticias_vacias, noticias_mal_descargadas, noticias_cortas, noticias_error_ln)

```


# Consigna 2) 

a) ¿Cuáles son los tópicos principales en el corpus?

b) ¿Pueden evidenciar diferencias en cada uno de los medios? Explicar qué método se utilizó para responder la pregunta, cuáles son los supuestos del mismo. 

c) Generar las visualizaciones más adecuadas para responder a las preguntas


# LDA


Si vemos los resultados ordenados por tf, se puede observar que existen 6 noticias que arrancan con "Últimas noticias de" y no contienen ninguna palabra (por eso la tf es 1: sólo ahí aparecen espacios vacíos). Deberíamos eliminarlos en el preprocesamiento.

Por otro lado, los resultados ordenados por tf_idf muestra que existen varias "noticias" que son en realidad avisos de descargas fallidas y sus palabras más características son básicamente links sin puntuación. También deben ser eliminados en el preprocesamiento.

```{r corpus_tf_idf_noticia}

# Creamos un corpus, pero esta vez centrados en los títulos
corpus_tf_idf_noticia <- corpus_noticias %>%
  count(titulo, word, 
        sort = TRUE) %>%
  bind_tf_idf(term = word,
              # Al poner como "document" el titulo, el idf expresa cuánto usa más un titulo que otro 
              # ciertas palabras
              document = titulo,
              n = n)

corpus_tf_idf_noticia |> 
  slice_max(n = 500,
            order_by = tf_idf,
            # En caso de empates ("tie"), por default la función trae ambos valores; lo seteamos
            # para que sólo traiga uno
            with_ties = FALSE) |> 
  print()

#rm(corpus_noticias)

```

Creación de la Document Term Matrix (DTM) del corpus de noticias

```{r corpus_dtm}

corpus_dtm <- corpus_tf_idf_noticia %>%
  cast_dtm(term = word,
           document = titulo, 
           value = n)

# La cantidad de documentos y de palabras parece coincidir con la del corpus
corpus_dtm |> 
  print()

```

Ejecutamos el LDA con 7 tópicos

```{r Prueba con 7 tópicos}
modelo_armado <- topicmodels::LDA(corpus_dtm, 
                              k = 7, 
                              control = list(seed = 1234567))
write_rds(modelo_armado, 'models/LDA7.rds')
```


```{r prob_x_palabra}

                  # Si esta línea les tira algún error, hagan install.packages("reshape2")
palabras_topicos <- tidy(modelo_armado, matrix = "beta") 


#Lo pasamos a porcentajes
palabras_topicos %>%
  mutate(beta = round(100*beta, 6))

```

Visualizamos las principales palabras por tópico en LDA

```{r top_palabras_topico}
#Agrupamos por tópicos y seleccionamos las principales 15 palabras de cada uno

top_palabras_topicos <- palabras_topicos %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta)

#Creamos el gáfico 

top_palabras_topicos %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()+
  labs(title = "Principales palabras por Tópico ",
       subtitle = "Según modelo LDA", 
       caption = "Gráfico n° 4",
  plot.title = element_text(size = 10))

```

Para observar qué títulos aparecen en las noticias del tópico 2 (el tópico menos claro)

```{r principales_noticias_x_topico}

topicos_noticias <- tidy(modelo_armado, matrix = "gamma")

topicos_noticias |>
  filter(topic == 2) %>%
  filter(gamma > 0.9) |> 
  mutate(gamma = round(gamma, 5)) |> 
  View()

```

Para observar cuáles noticias no parecen encajar bien en ninguno de los tópicos posibles

```{r noticias_poco_clasificables}

topicos_noticias |> 
  # Ponemos un umbral de bajas chances de encajar en cualquier tópico
  filter(gamma < 0.3) |> 
  # Contamos cuántos son los documentos que cumplen la condición anterior (porque si encajan 
  # perfecto en un tópico, es obvio que en los otros no encajan)
  count(document) |> 
  # Si aparece 5 veces el titulo, es que en los 7 tópicos tiene menos de un 30% de chances
  filter(n == 7) |> 
  View()

```

Armamos un gráfico LDA para observar qué tópicos tienen mayor representación en cada medio

```{r topicos_x_medio}

topicos_noticias %>%
  rename(titulo = document) %>% 
  left_join(noticias %>% select(titulo, medio) %>% unique()) %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)*100) %>%
  ggplot() +
    geom_col(aes(x = factor(topic),
                 y = mean,
                 fill = medio), 
             position='dodge') +
    theme_minimal() +
    theme(legend.position = "bottom",  # Colocar la leyenda abajo del gráfico
          legend.text=element_text(size=rel(0.8)),  # Achicar el texto de la leyenda
          legend.title=element_text(size=rel(0.8)),  # Achicar el título de la leyenda
          axis.text.x = element_text(angle = 0, hjust = 1)) +  # Rotar y ajustar texto del eje x
    labs(title = "Tópicos por medio", 
          subtitle = "Según modelo LDA", 
       caption = "Gráfico n° 5")  

```


# Revisar nombre de los tópicos LDA 7

Tópico 1: Justicia y seguridad

Tópico 2: Vida y Relaciones Personales 
(vida, casa, gente, historia, mujer, familia, mundo, amor, vídeo, película, programa, música, hijo, padre)

Tópico 3: Sociedad y Bienestar 
(mujeres, personas, salud, vida, argentina, mundo, social, educación, país, sociedad, sistema, sociales, política, forma, nacional)

Tópico 4: Economía y Política Internacional (millones, gobierno, país, argentina, dólares, acuerdo, presidente, Trump, unidos, mercado, ciento, pesos, dólar, banco, economía)

Tópico 5: Política Nacional y Elecciones (
Macri, Fernández, gobierno, presidente, frente, Alberto, candidato, Cristina, campaña, Argentina, paso, país, elecciones, nacional, Mauricio)

Tópico 6: Problemas Urbanos y Ambientales (ciudad, personas, Aires, Argentina, país, agua, mundo, productos, nacional, san, zona, centro, sistema, metros, forma)

Tópico 7: Deportes y Entretenimiento (equipo, partido, final, Argentina, Boca, copa, fútbol, club, River, mundial, argentino, selección, minutos, jugadores, mundo)



# STM

```{r}
word_counts <- corpus_noticias %>%  
        group_by(id,word) %>%
        summarise(n=n()) %>%
        ungroup()

noticias_dfm <- word_counts %>%
                cast_dfm(id, word, n)

noticias_dfm
```


```{r message=FALSE, warning=FALSE}
library(readr)
# stm_7 <- read_rds('models/stm_7_prev_cont.rds'')

metadata <- corpus_noticias %>%  
        group_by(id,medio ) %>%
        summarise(n=n()) %>%
        ungroup()

stm_7 <- stm(documents = noticias_dfm,
      K = 7,
      prevalence = ~medio,
      max.em.its = 75, 
      data = metadata,
      init.type = "Spectral")
write_rds(stm_7, 'models/stm_7_prev_cont.rds')

```



```{r}
betas_stm <- tidy(stm_7, matrix='beta')
tetha_stm <- tidy(stm_7, matrix='theta')

betas_stm %>%
  group_by(topic) %>%
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  arrange(topic, -beta) %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales='free_y') +
  scale_y_reordered() +
  theme_minimal()+
  labs(title = "Principales palabras por Tópico",
       subtitle = "Según modelo STM", 
       caption = "Gráfico n° 6",
  plot.title = element_text(size = 10))


```

```{r}
theta_stm_7 <- tetha_stm %>%
  rename(id=document) %>%
  left_join(metadata)
 
```

```{r}
theta_stm_7 %>%
  group_by(medio, topic) %>%
  summarise(mean = mean(gamma)) %>%
  drop_na() %>%
  ggplot(aes(x=medio, y=mean, fill=medio)) + 
    geom_col(position='dodge') +
    facet_wrap(~topic) +
    theme_minimal() +
    theme(legend.position = "bottom",  # Colocar la leyenda debajo del gráfico
          axis.text.x = element_blank()) +  # Quitar la leyenda del eje x
    labs(title = "Principales palabras por Tópico según medio",
          subtitle = "Según modelo STM", 
       caption = "Gráfico n° 7",  
         plot.title = element_text(size = 14))  # Ajustar el tamaño del título del gráfico

 
```

```{r}
labelTopics(stm_7)
```


Tópico 1: Política/elecciones
Tópico 2: Política exterior
Tópico 3: Economía
Tópico 4: Astrología, salud y ciencia 
Tópico 5: Justicia y seguridad
Tópico 6: Espectáculo
Tópico 7: Deportes

CHEQUEAR COMPARACION del modelo LDA contra el modelo STM

# Consigna 3) 

A continuación, seleccionar las noticias vinculadas a algún tópico relevante (por
ejemplo, “Elecciones”) y construir un clasificador para predecir la orientación del
diario1. 
Utilizar alguno de los modelos de clasificación vistos a lo largo de al
Diplomatura (regresión logística, random forest, etc.). 
Utilizar como features el
“Spanish Billion Word Corpus and Embeddings”, analizado en clase (pueden
descargar el embedding en formato .bin del link). 

¿Qué resultados arroja el modelo? ¿Es posible mediante el texto de las noticias conocer la línea editorial del diario? 
Generar las visualizaciones y tablas correspondientes para una correcta evaluación del modelo.

```{r}
#TRABAJE CON EL TOPICO 5 DE LDA QUE ES "ELECCIONES/POLÍTICA NACIONAL"
#Importante: La variable objetivo orientación debo convertirla en factor para correr una regresión logistica (es decir, que sea una variable con categorías). Por otra parte, si van a utilizar funciones que calculan métricas derivadas de la matriz de confusión es mucho muy importante que las categorías estén ordenadas con el valor positivo como primer nivel y el negativo como el segundo nivel (ver que esto no lo he hecho, 
#base_para_modelar <- base_juguete %>%   #mutate(realiza_trabajo_domestico = factor(realiza_trabajo_domestico,levels = c("Si","No")

corpus_topic <- topicos_noticias %>% ##IGNACIO!!! acA ES DONDE HICE EL JOIN CON EL CORPUS TOTAL
  rename(titulo = document) %>%
  left_join(
    noticias %>%
    #distinct(titulo, .keep_all = TRUE) %>%
    select(titulo, medio, orientacion, texto)
  ) %>%
  mutate(orientacion = gsub("\\+ ", "", orientacion))%>% 
  mutate(orientacion = as.factor(orientacion)) %>%
  group_by(medio, topic) %>%
  ungroup() 

```

   
```{r topicos_x_medio}
#acA FILTRE EL TOPICO 5 de LDA QUE ES ELECCIONES, PERO SIN HACER SELECCIONAR LAS NOTICIAS QUE TIENEN GAMA MAS ALTO DE ESE TOPICO
corpus_Topic5 <- corpus_topic %>%
  filter(topic == 5) %>%
    select(-gamma, -topic)

```


```{r prueba con muestra del 50}

# Obtener una muestra del 50%ACa habia trabajado con una muestra del 50% porque sino se vuelve muy pesado. Yo creo que si hacemos las operaciones anteriores no es necesario.
corpus_Topic5_muestra <- corpus_Topic5 %>%
  sample_frac(0.5, replace = FALSE)

# Ahora corpus_Topic5_muestra contiene el 50% aleatorio de los datos originales.

```

#### Aca empieza la clase 6
```{r}
## Word embeddings como features. Realizamos un preprocesamiento siempre. No vamos a pasar a minúscula nada y tampoco vamos a eliminar caracteres no ascii.
noticias_embed <- corpus_Topic5_muestra %>%
        mutate(texto = str_replace_all(texto, "'\\[.*?¿\\]\\%'", " ")) %>%
        mutate(texto = str_replace_all(texto, "[[:digit:]]+", "DIGITO"))
```

Carga del embedding

Vamos a usar un embedding entrenado mediante el algoritmo wor2vec por C. Cardelino. Está entrenado sobre un corpus grande en español. Pueden encontrar los detalles [aquí](https://crscardellino.ar/SBWCE/).

Escribamos una función que cargue el embedding y ejecutémosla:
```{r}
load_embeddings <- function(path=NULL, type=c("w2v", "ft")){
        if (type=="w2v"){
                embedding <- word2vec::read.wordvectors(path, 
                                                        type = "bin", 
                                                        normalize = TRUE) %>%
                        as_tibble(rownames="word")
        }
        else if (type=="ft"){
                model <- fastTextR::ft_load(path)
                words <- fastTextR::ft_words(model)
                embedding <- fastTextR::ft_word_vectors(model,
                                                        words) %>%
                        as_tibble(rownames="word")
        }
        
        return(embedding)
}


embedding <- load_embeddings(path = "WordEmbeddings/Word2Vec/sbw_vectors.bin",
                             type = "w2v")

```
### Tokenización y construcción de features mediante el embedding
Si bien existe un método `step_word_embeddings()` análogo al `step_tfidf()` que realizaría la vectorización, vamos a hacer "a mano" para tratar de entender qué pasa (y porque además, el método tiene algunos comportamientos raros).

La secuencia de operaciones que vamos a realizar es la siguiente:

1. Tokenización pero sin pasar todo a minúscula y sin eliminar la puntuación.
```{r}


noticias_tidy <- noticias_embed %>%
  mutate(title_id = row_number()) %>%
  unnest_tokens(word, texto, 
                to_lower=FALSE, #estaba en TRUE
                strip_punct=TRUE) 

```
2. Left-join de la tabla tokenizada de cada noticia con el embedding. De esta forma, cada palabra va a estar representada por un vector de 300 dimensiones, que se corresponde con un vector del embedding pre-entrenado.
```{r}

#
noticias_tidy <- noticias_tidy %>%
        left_join(embedding) %>%
        drop_na()
```
3. Por último, agrupamos por cada noticia y calculamos el promedio para cada dimensión de cada palabra que forma parte de una noticia.
```{r}
tictoc::tic()
noticias_embed <- noticias_tidy %>%
        group_by(title_id,orientacion) %>%
        summarise(across(V1:V300, ~mean(.x, na.rm=TRUE))) %>%
        ungroup()
tictoc::toc()
```
Luego, dividimos los datos en conjuntos de entrenamiento y prueba. Podemos utilizar la función initial_split() de rsample para crear esta división binaria de los datos. El argumento orientación garantiza que la distribución del producto sea similar en el conjunto de entrenamiento y el conjunto de prueba. Dado que la división utiliza un muestreo aleatorio, establecemos una semilla para poder reproducir nuestros resultados.

```{r}
## Split
set.seed(1234)
noticias_split <- initial_split(noticias_embed, strata = orientacion)

train_embed <- training(noticias_split)
test_embed <- testing(noticias_split)

#ver el balanceo en train
table (train_embed$orientacion) #para ver el balanceo)

#Conservador 44,5%, progresista 20%, neutro 35.5$

```
## Hiperparámetros

Ahora, hay que generar el modelo de random forest. Esto lo hacemos con la función `rand_forest()`. Los hiperparámetros con los que vamos a jugar acá son:

-   `trees`: define el número de árboles que se van a probar,
-   `mtry`: la cantidad de variables que se van a probar en cada iteración
-   `min_n`: mínima cantidad de observaciones que tiene que haber en cada nodo

```{r}
rf_spec <- rand_forest(
  trees = 1000,
  mtry = tune(),
  min_n = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("ranger")

```
A continuación, necesitamos preprocesar estos datos para prepararlos para el modelado; tenemos datos de texto y necesitamos construir características numéricas para el aprendizaje automático a partir de ese texto.

El paquete `recipes`, que forma parte de `tidymodels`, nos permite crear una especificación de los pasos de preprocesamiento que queremos realizar. Estas transformaciones se estiman (o "entrenan") en el conjunto de entrenamiento para que puedan aplicarse de la misma manera en el conjunto de prueba o en nuevos datos durante la predicción, sin fuga de datos. Inicializamos nuestro conjunto de transformaciones de preprocesamiento con la función `recipe()`, utilizando una expresión de fórmula para especificar las variables, nuestra variable de resultado junto con nuestro predictor, junto con el conjunto de datos.

```{r}


# especifico la receta
noticias_rec_embed <-
        recipe(orientacion ~ ., data = train_embed) %>%
        update_role("title_id", new_role = "ID")%>%
        step_downsample(orientacion, under_ratio = 1) #esto sería para balancear las clases


# especifico el flujo
wf_embed <- workflow() %>% 
        add_recipe(noticias_rec_embed) %>%
        add_model(rf_spec)

# Seteo de validación cruzada
set.seed(1234)#ver si tengo que usar la misma semila que esta arriba
embed_folds <- vfold_cv(train_embed, v = 5)


```

### Entrenamiento
```{r}
# Entreno el modelo
tune_params <- tune_grid(wf_embed,
                         resamples = embed_folds,
                         grid = 10,#estaba en 30, demora muchisimo
                         metrics = metric_set(precision, recall,
                                              roc_auc, f_meas))

write_rds(tune_params, 'models/tune_params.rds')

```

```{r modelo_topicos}

tune_params <- read_rds('models/tune_params.rds')


```
#VER completo lo que sigue


```{r}

# Resumen de los resultados
summary(tune_params)

# Recopilar métricas en un tibble
metrics_df <- collect_metrics(tune_params) ####VER! 

```


```{r}
autoplot(tune_params)+
  theme_minimal()
```


### Entrenamiento
```{r}
# Entreno el modelo # esto ya lo habría hecho arriba
#tictoc::tic()
#tune_lasso_embed <- tune_grid(
 #       wf_embed,
 #       embed_folds,
 #       grid = grid_lasso,
#        control = control_resamples(save_pred = TRUE)
#)
tictoc::toc()
```

### Evaluación
Veamos los dos mejores modelos en términos de ROC:
```{r}
show_best(tune_params, "roc_auc")
```
Veamos el mejor modelo dentro de 1 error estándar:
```{r}
best_ROC <- select_best(tune_params, "roc_auc")

final_rf <- finalize_model(
  rf_spec,
  best_ROC
)

final_rf

```

```{r}
# Crear un nuevo flujo de trabajo, agregar la receta y el modelo, y ajustar al conjunto de datos de entrenamiento en una sola línea
tree_fit <- workflow() %>%
  add_recipe(noticias_rec_embed) %>%
  add_model(final_rf) %>%
  fit(data = train_embed)
```
¿Cómo es la performance?

```{r}
test_val <- tree_fit %>%
  predict(test_embed) %>%
  bind_cols(., test_embed)


test_val <- predict(tree_fit, test_embed, type = "prob") %>%
  bind_cols(test_val, .)

#evaluation_metrics <- test_val %>%
  #roc_auc(orientacion, .pred_conservador) %>%
  #bind_rows(accuracy(orientacion, .pred_class)) %>%
  #bind_rows(precision(orientacion, .pred_class)) %>%
  #bind_rows(recall(orientacion, .pred_class)) %>%
  #bind_rows(f_meas(orientacion, .pred_class))




```

¿Cómo son los resultados en comparación con los otros 2 modelos?

```{r}


preds_embed <- test_embed %>%
        select(orientacion) %>%
        bind_cols(predict(tree_fit, test_embed, type="prob")) %>%
        bind_cols(predict(tree_fit, test_embed, type="class"))

preds_embed <- preds_embed %>% 
               mutate(orientacion = gsub("\\+ ", "", orientacion)) %>%
               mutate(orientacion = as.factor(orientacion)) %>%
               rename_all(~gsub("\\+ ", "", .)) %>%
               mutate(across(starts_with(".pred_"), ~gsub("\\+ ", "", .))) %>%
             
print (preds_embed)

```



**Embeddings**
```{r}
#este es el que da error
roc_auc(preds_embed, orientacion, .pred_conservador) %>%
bind_rows(accuracy(preds_embed, orientacion, .pred_class)) %>%
bind_rows(precision(preds_embed, orientacion, .pred_class)) %>%
bind_rows(recall(preds_embed, orientacion, .pred_class)) %>%
bind_rows(f_meas(preds_embed, orientacion, .pred_class))
```
##Pruebas con Regresión logistica LASSO, una vez que funcione esto se puede ser con LASSO. Debemos trabajar con una modificacion de la variable "orientacion" hacia una variable dicotomcia.  mutate(orientacion = ifelse(orientacion %in% c('progresista', 'neutro'), 'no conservador', 'conservador'))
